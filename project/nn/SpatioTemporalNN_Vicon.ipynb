{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "udHK8gS5gSHv",
    "ExecuteTime": {
     "end_time": "2024-03-18T22:52:19.803635Z",
     "start_time": "2024-03-18T22:52:19.715502Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 12\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\u001B[38;5;241m,\u001B[39m\u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# The code is run on a CPU\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Input, Conv1D, LSTM, Dense, Dropout, Activation, Flatten, concatenate, UpSampling1D\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EarlyStopping\n",
      "File \u001B[0;32m~/work/graduation/automatic_evaluation_of_physical_therapy_exercises_based_on_deep_learning/.venv/lib/python3.11/site-packages/keras/__init__.py:8\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"DO NOT EDIT.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03msince your modifications would be overwritten.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _tf_keras\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n",
      "File \u001B[0;32m~/work/graduation/automatic_evaluation_of_physical_therapy_exercises_based_on_deep_learning/.venv/lib/python3.11/site-packages/keras/_tf_keras/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_tf_keras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n",
      "File \u001B[0;32m~/work/graduation/automatic_evaluation_of_physical_therapy_exercises_based_on_deep_learning/.venv/lib/python3.11/site-packages/keras/_tf_keras/keras/__init__.py:8\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"DO NOT EDIT.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03msince your modifications would be overwritten.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m callbacks\n",
      "File \u001B[0;32m~/work/graduation/automatic_evaluation_of_physical_therapy_exercises_based_on_deep_learning/.venv/lib/python3.11/site-packages/keras/activations/__init__.py:8\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"DO NOT EDIT.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03msince your modifications would be overwritten.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deserialize\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m serialize\n",
      "File \u001B[0;32m~/work/graduation/automatic_evaluation_of_physical_therapy_exercises_based_on_deep_learning/.venv/lib/python3.11/site-packages/keras/src/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n",
      "File \u001B[0;32m~/work/graduation/automatic_evaluation_of_physical_therapy_exercises_based_on_deep_learning/.venv/lib/python3.11/site-packages/keras/src/activations/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtypes\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m elu\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m exponential\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gelu\n",
      "File \u001B[0;32m~/work/graduation/automatic_evaluation_of_physical_therapy_exercises_based_on_deep_learning/.venv/lib/python3.11/site-packages/keras/src/activations/activations.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi_export\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras_export\n",
      "File \u001B[0;32m~/work/graduation/automatic_evaluation_of_physical_therapy_exercises_based_on_deep_learning/.venv/lib/python3.11/site-packages/keras/src/backend/__init__.py:33\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Import backend functions.\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m backend() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensorflow\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 33\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa: F403\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m backend() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjax\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa: F403\u001B[39;00m\n",
      "File \u001B[0;32m~/work/graduation/automatic_evaluation_of_physical_therapy_exercises_based_on_deep_learning/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribution_lib\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m image\n",
      "File \u001B[0;32m~/work/graduation/automatic_evaluation_of_physical_therapy_exercises_based_on_deep_learning/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtypes\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompiler\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtf2xla\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mxla\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dynamic_update_slice\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KerasVariable\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Import libraries and functions\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import csv\n",
    "import os,random\n",
    "\n",
    "# The code is run on a CPU\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, LSTM, Dense, Dropout, Activation, Flatten, concatenate, UpSampling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import *\n",
    "from keras.layers import Lambda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "2nmED41Do4X8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "timesteps = 240  # number of timesteps\n",
    "nr = 90   # number of repetitions\n",
    "n_dim = 117  # dimension of the data sequences"
   ],
   "metadata": {
    "id": "XT04dR2piL19"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**DataViconLoad**"
   ],
   "metadata": {
    "id": "ouoSAGTuq1Kf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "This function is called to load the data for the firsr exercise Deep Squat when CNN_GMM_Between_M1 is running\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# time steps\n",
    "timesteps = 240\n",
    "# repetition number\n",
    "nr = 90\n",
    "\n",
    "data_correct_path = \"data/Data_Correct.csv\"\n",
    "data_incorrect_path = \"data/Data_Incorrect.csv\"\n",
    "\n",
    "labels_correct_path = \"data/Labels_Correct.csv\"\n",
    "labels_incorrect_path = \"data/Labels_Incorrect.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# /content/drive/MyDrive/Data\n",
    "def load_data():\n",
    "    f = open(data_correct_path)\n",
    "    csv_f = csv.reader(f)\n",
    "    Correct_X = list(csv_f)\n",
    "\n",
    "    # Convert the input sequences into numpy arrays\n",
    "    train_input1 = np.asarray(Correct_X, dtype = float)\n",
    "    n_dim = 117\n",
    "    correct_input = np.zeros((nr,timesteps,n_dim))\n",
    "    for i in range(len(train_input1)//n_dim):\n",
    "           correct_input[i,:,:] = np.transpose(train_input1[n_dim*i:n_dim*(i+1),:])\n",
    "\n",
    "    f = open(labels_correct_path)\n",
    "    csv_f = csv.reader(f)\n",
    "    Correct_Y = list(csv_f)\n",
    "\n",
    "    # Convert the input labels into numpy arrays\n",
    "    correct_label = np.asarray(Correct_Y, dtype = float)\n",
    "\n",
    "    f = open(data_incorrect_path)\n",
    "    csv_f = csv.reader(f)\n",
    "    Incorrect_X = list(csv_f)\n",
    "\n",
    "    # Convert the input sequences into numpy arrays\n",
    "    test_input1 = np.asarray(Incorrect_X)\n",
    "    n_dim = 117\n",
    "    incorrect_input = np.zeros((nr,timesteps,n_dim))\n",
    "    for i in range(len(test_input1)//n_dim):\n",
    "          incorrect_input[i,:,:] = np.transpose(test_input1[n_dim*i:n_dim*(i+1),:])\n",
    "\n",
    "    f = open(labels_incorrect_path)\n",
    "    csv_f = csv.reader(f)\n",
    "    Incorrect_Y = list(csv_f)\n",
    "\n",
    "    # Convert the input labels into numpy arrays\n",
    "    incorrect_label = np.asarray(Incorrect_Y, dtype = float)\n",
    "\n",
    "    return correct_input, correct_label, incorrect_input, incorrect_label"
   ],
   "metadata": {
    "id": "Jy8Q1xlEqzH4",
    "ExecuteTime": {
     "end_time": "2024-03-18T22:46:43.990383Z",
     "start_time": "2024-03-18T22:46:43.934301Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import DataViconLoad   # Import the data\n",
    "Correct_data, Correct_label, Incorrect_data, Incorrect_label = load_data()\n",
    "\n",
    "# Print the size of the data\n",
    "print(Correct_data.shape, 'correct sequences')\n",
    "print(Correct_label.shape, 'correct labels')\n",
    "print(Incorrect_data.shape, 'incorrect sequences')\n",
    "print(Incorrect_label.shape, 'incorrect labels')"
   ],
   "metadata": {
    "id": "vCRxIw1qiO4w"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the data into training and validation sets\n",
    "# Training set: 70%\n",
    "# Validation set: 30%\n",
    "\n",
    "# Sample random indices\n",
    "import random\n",
    "\n",
    "# variable that holds the total number of repetitions (Correct-Incorrect)\n",
    "trainidx1 = random.sample(range(0,Correct_data.shape[0]),int(nr*0.7))\n",
    "trainidx2 = random.sample(range(0,Incorrect_data.shape[0]),int(nr*0.7))\n",
    "valididx1 = np.setdiff1d(np.arange(0,nr,1),trainidx1)\n",
    "valididx2 = np.setdiff1d(np.arange(0,nr,1),trainidx2)\n",
    "\n",
    "# Training set: data and labels\n",
    "train_x = np.concatenate((Correct_data[trainidx1,:,:],Incorrect_data[trainidx2,:,:]))\n",
    "print(train_x.shape, 'training data')\n",
    "train_y = np.concatenate((np.squeeze(Correct_label[trainidx1]),np.squeeze(Incorrect_label[trainidx2])))\n",
    "print(train_y.shape, 'training labels')\n",
    "\n",
    "# Validation set: data and labels\n",
    "# Print the shape (output: (number of samples (recordings),number of rows, number of columns ))\n",
    "valid_x = np.concatenate((Correct_data[valididx1,:,:],Incorrect_data[valididx2,:,:]))\n",
    "print(valid_x.shape, 'validation data')\n",
    "valid_y = np.concatenate((np.squeeze(Correct_label[valididx1]),np.squeeze(Incorrect_label[valididx2])))\n",
    "print(valid_y.shape, 'validation labels')"
   ],
   "metadata": {
    "id": "HuqvqHhtmNn7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the first and last sequence in the training and validation sets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (14,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(train_x[0])\n",
    "plt.title('First Train Sequence')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(train_x[-1])\n",
    "plt.title('Last Train Sequence')\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(valid_x[0])\n",
    "plt.title('First Validation Sequence')\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(valid_x[-1])\n",
    "plt.title('Last Validation Sequence')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "3ZDDgGLF7oLM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reduce the data length by a factor of 2, 4, and 8\n",
    "# The reduced sequences will be used as inputs to the temporal pyramid subnetwork\n",
    "train_x_2 = np.zeros((train_x.shape[0], int(train_x.shape[1]/2), train_x.shape[2]))\n",
    "valid_x_2 = np.zeros(train_x_2.shape)\n",
    "train_x_4 = np.zeros((train_x.shape[0], int(train_x.shape[1]/4), train_x.shape[2]))\n",
    "valid_x_4 = np.zeros(train_x_4.shape)\n",
    "train_x_8 = np.zeros((train_x.shape[0], int(train_x.shape[1]/8), train_x.shape[2]))\n",
    "valid_x_8 = np.zeros(train_x_8.shape)\n",
    "train_x_2 = train_x[:,::2,:]\n",
    "valid_x_2 = valid_x[:,::2,:]\n",
    "train_x_4 = train_x[:,::4,:]\n",
    "valid_x_4 = valid_x[:,::4,:]\n",
    "train_x_8 = train_x[:,::8,:]\n",
    "valid_x_8 = valid_x[:,::8,:]"
   ],
   "metadata": {
    "id": "eH9WON0s8RYS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Code to re-order the 117 dimensional skeleton data from the Vicon optical tracker into trunk, left arm, right arm, left leg and right leg\n",
    "def reorder_data(x):\n",
    "  #X_trunk: This array will hold data specifically for the torso/trunk joints.\n",
    "    X_trunk = np.zeros((x.shape[0],x.shape[1],12))\n",
    "    X_left_arm = np.zeros((x.shape[0],x.shape[1],18))\n",
    "    X_right_arm = np.zeros((x.shape[0],x.shape[1],18))\n",
    "    X_left_leg = np.zeros((x.shape[0],x.shape[1],21))\n",
    "    X_right_leg = np.zeros((x.shape[0],x.shape[1],21))\n",
    "    X_trunk =  np.concatenate((x[:,:,15:18], x[:,:,18:21], x[:,:,24:27], x[:,:,27:30]), axis = 2)\n",
    "    X_left_arm = np.concatenate((x[:,:,81:84], x[:,:,87:90], x[:,:,93:96], x[:,:,99:102], x[:,:,105:108], x[:,:,111:114]), axis = 2)\n",
    "    X_right_arm = np.concatenate((x[:,:,84:87], x[:,:,90:93], x[:,:,96:99], x[:,:,102:105], x[:,:,108:111], x[:,:,114:117]), axis = 2)\n",
    "    X_left_leg = np.concatenate((x[:,:,33:36], x[:,:,39:42], x[:,:,45:48], x[:,:,51:54], x[:,:,57:60], x[:,:,63:66], x[:,:,69:72]), axis = 2)\n",
    "    X_right_leg = np.concatenate((x[:,:,36:39], x[:,:,42:45], x[:,:,48:51], x[:,:,54:57], x[:,:,60:63], x[:,:,66:69], x[:,:,72:75]), axis = 2)\n",
    "    #combines the five body part arrays\n",
    "    x_segmented = np.concatenate((X_trunk, X_right_arm, X_left_arm, X_right_leg, X_left_leg),axis = -1)\n",
    "    return x_segmented"
   ],
   "metadata": {
    "id": "HukHCM758S8m"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reorder the data dimensions to correspond to the five body parts\n",
    "trainx =  reorder_data(train_x)\n",
    "validx =  reorder_data(valid_x)\n",
    "trainx_2 =  reorder_data(train_x_2)\n",
    "validx_2 =  reorder_data(valid_x_2)\n",
    "trainx_4 =  reorder_data(train_x_4)\n",
    "validx_4 =  reorder_data(valid_x_4)\n",
    "trainx_8 =  reorder_data(train_x_8)\n",
    "validx_8 =  reorder_data(valid_x_8)"
   ],
   "metadata": {
    "id": "eAwwx50x8Xaj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define a multibranch convolutional Inception-like block\n",
    "def MultiBranchConv1D(input , filters1 , kernel_size1 , strides1 , strides2):\n",
    "\n",
    "    #kernel_size allows each branch to capture features at different scales within the input data.\n",
    "    x1 = Conv1D(filters=filters1, kernel_size=kernel_size1+2, strides=strides1, padding='same', activation='relu')(input)\n",
    "    x1 = Dropout(0.25)(x1)\n",
    "    x2 = Conv1D(filters=filters1, kernel_size=kernel_size1+6, strides=strides1, padding='same', activation='relu')(input)\n",
    "    x2 = Dropout(0.25)(x2)\n",
    "    x3 = Conv1D(filters=filters1, kernel_size=kernel_size1+12, strides=strides1, padding='same', activation='relu')(input)\n",
    "    x3 = Dropout(0.25)(x3)\n",
    "    y1 = concatenate([x1, x2, x3], axis=-1)\n",
    "\n",
    "    x4 = Conv1D(filters=filters1, kernel_size=kernel_size1, strides=strides2, padding='same', activation='relu')(y1)\n",
    "    #Dropout is applied after each convolution to prevent overfitting.\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x5 = Conv1D(filters=filters1, kernel_size=kernel_size1+2, strides=strides2, padding='same', activation='relu')(y1)\n",
    "    x5 = Dropout(0.25)(x5)\n",
    "    x6 = Conv1D(filters=filters1, kernel_size=kernel_size1+4, strides=strides2, padding='same', activation='relu')(y1)\n",
    "    x6 = Dropout(0.25)(x6)\n",
    "    x = concatenate([x4, x5, x6], axis=-1)\n",
    "    return x"
   ],
   "metadata": {
    "id": "FIZwT37B8YPV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define a temporal pyramid network\n",
    "def TempPyramid(input_f, input_2, input_4, input_8, seq_len, n_dims):\n",
    "\n",
    "    #### Full scale sequences\n",
    "    conv1 = MultiBranchConv1D(input_f, 64, 3, 2, 2)\n",
    "\n",
    "    #### Half scale sequences\n",
    "    conv2 = MultiBranchConv1D(input_2, 64, 3, 2, 1)\n",
    "\n",
    "    #### Quarter scale sequences\n",
    "    conv3 = MultiBranchConv1D(input_4, 64, 3, 1, 1)\n",
    "\n",
    "    #### Eighth scale sequences\n",
    "    conv4 = MultiBranchConv1D(input_8, 64, 3, 1, 1)\n",
    "    upsample1 = UpSampling1D(size = 2)(conv4)\n",
    "\n",
    "    #### Recurrent layers\n",
    "    x = concatenate([conv1, conv2, conv3, upsample1], axis=-1)\n",
    "    return x"
   ],
   "metadata": {
    "id": "Ftg2weLr8b-N"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.layers import Input, Lambda, UpSampling1D, LSTM, Dense, Input, concatenate, Conv1D, Dropout\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import EarlyStopping"
   ],
   "metadata": {
    "id": "XG3e_-o7ujm-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def MultiBranchConv1D(input, filters1, kernel_size1, strides1, strides2):\n",
    "    x1 = Conv1D(filters=filters1, kernel_size=kernel_size1+2, strides=strides1, padding='same', activation='relu')(input)\n",
    "    x1 = Dropout(0.25)(x1)\n",
    "    x2 = Conv1D(filters=filters1, kernel_size=kernel_size1+6, strides=strides1, padding='same', activation='relu')(input)\n",
    "    x2 = Dropout(0.25)(x2)\n",
    "    x3 = Conv1D(filters=filters1, kernel_size=kernel_size1+12, strides=strides1, padding='same', activation='relu')(input)\n",
    "    x3 = Dropout(0.25)(x3)\n",
    "    y1 = concatenate([x1, x2, x3], axis=-1)\n",
    "\n",
    "    x4 = Conv1D(filters=filters1, kernel_size=kernel_size1, strides=strides2, padding='same', activation='relu')(y1)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x5 = Conv1D(filters=filters1, kernel_size=kernel_size1+2, strides=strides2, padding='same', activation='relu')(y1)\n",
    "    x5 = Dropout(0.25)(x5)\n",
    "    x6 = Conv1D(filters=filters1, kernel_size=kernel_size1+4, strides=strides2, padding='same', activation='relu')(y1)\n",
    "    x6 = Dropout(0.25)(x6)\n",
    "    x = concatenate([x4, x5, x6], axis=-1)\n",
    "    return x\n",
    "\n",
    "n_dim = 90 # dimension after segmenting the data into body parts\n",
    "n_dim1 = 12 # trunk dimension\n",
    "n_dim2 = 18 # arms dimension\n",
    "n_dim3 = 21 # legs dimension\n",
    "\n",
    "# Build the model ...\n",
    "\n",
    "#### Full scale sequences\n",
    "seq_input = Input(shape = (timesteps, n_dim), name = 'full_scale')\n",
    "\n",
    "seq_input_trunk = Lambda(lambda x: x[:, :, 0:12])(seq_input)\n",
    "seq_input_left_arm = Lambda(lambda x: x[:, :, 12:30])(seq_input)\n",
    "seq_input_right_arm = Lambda(lambda x: x[:, :, 30:48])(seq_input)\n",
    "seq_input_left_leg = Lambda(lambda x: x[:, :, 48:69])(seq_input)\n",
    "seq_input_right_leg = Lambda(lambda x: x[:, :, 69:90])(seq_input)\n",
    "\n",
    "#### Half scale sequences\n",
    "seq_input_2 = Input(shape=(int(timesteps/2), n_dim), name='half_scale')\n",
    "\n",
    "seq_input_trunk_2 = Lambda(lambda x: x[:, :, 0:12])(seq_input_2)\n",
    "seq_input_left_arm_2 = Lambda(lambda x: x[:, :, 12:30])(seq_input_2)\n",
    "seq_input_right_arm_2 = Lambda(lambda x: x[:, :, 30:48])(seq_input_2)\n",
    "seq_input_left_leg_2 = Lambda(lambda x: x[:, :, 48:69])(seq_input_2)\n",
    "seq_input_right_leg_2 = Lambda(lambda x: x[:, :, 69:90])(seq_input_2)\n",
    "\n",
    "#### Quarter scale sequences\n",
    "seq_input_4 = Input(shape=(int(timesteps/4), n_dim), name='quarter_scale')\n",
    "\n",
    "seq_input_trunk_4 = Lambda(lambda x: x[:, :, 0:12])(seq_input_4)\n",
    "seq_input_left_arm_4 = Lambda(lambda x: x[:, :, 12:30])(seq_input_4)\n",
    "seq_input_right_arm_4 = Lambda(lambda x: x[:, :, 30:48])(seq_input_4)\n",
    "seq_input_left_leg_4 = Lambda(lambda x: x[:, :, 48:69])(seq_input_4)\n",
    "seq_input_right_leg_4 = Lambda(lambda x: x[:, :, 69:90])(seq_input_4)\n",
    "\n",
    "#### Eighth scale sequences\n",
    "seq_input_8 = Input(shape=(int(timesteps/8), n_dim), name='eighth_scale')\n",
    "\n",
    "seq_input_trunk_8 = Lambda(lambda x: x[:, :, 0:12])(seq_input_8)\n",
    "seq_input_left_arm_8 = Lambda(lambda x: x[:, :, 12:30])(seq_input_8)\n",
    "seq_input_right_arm_8 = Lambda(lambda x: x[:, :, 30:48])(seq_input_8)\n",
    "seq_input_left_leg_8 = Lambda(lambda x: x[:, :, 48:69])(seq_input_8)\n",
    "seq_input_right_leg_8 = Lambda(lambda x: x[:, :, 69:90])(seq_input_8)\n",
    "\n",
    "concat_trunk = TempPyramid(seq_input_trunk, seq_input_trunk_2, seq_input_trunk_4, seq_input_trunk_8, timesteps, n_dim1)\n",
    "concat_left_arm = TempPyramid(seq_input_left_arm, seq_input_left_arm_2, seq_input_left_arm_4, seq_input_left_arm_8, timesteps, n_dim2)\n",
    "concat_right_arm = TempPyramid(seq_input_right_arm, seq_input_right_arm_2, seq_input_right_arm_4, seq_input_right_arm_8, timesteps, n_dim2)\n",
    "concat_left_leg = TempPyramid(seq_input_left_leg, seq_input_left_leg_2, seq_input_left_leg_4, seq_input_left_leg_8, timesteps, n_dim3)\n",
    "concat_right_leg = TempPyramid(seq_input_right_leg, seq_input_right_leg_2, seq_input_right_leg_4, seq_input_right_leg_8, timesteps, n_dim3)\n",
    "\n",
    "concat = concatenate([concat_trunk, concat_left_arm, concat_right_arm, concat_left_leg, concat_right_leg], axis=-1)\n",
    "\n",
    "rec = LSTM(80, return_sequences=True)(concat)\n",
    "rec1 = LSTM(40, return_sequences=True)(rec)\n",
    "rec1 = LSTM(40, return_sequences=True)(rec1)\n",
    "rec2 = LSTM(80)(rec1)\n",
    "\n",
    "out = Dense(1, activation = 'sigmoid')(rec2)\n",
    "\n",
    "model = Model(inputs=[seq_input, seq_input_2, seq_input_4, seq_input_8], outputs=out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer= Adam(lr=0.0001))"
   ],
   "metadata": {
    "id": "uCb5u47J8fCK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "t = datetime.now()\n",
    "\n",
    "# Create the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25)\n",
    "\n",
    "# ...\n",
    "\n",
    "# Fit the model with the EarlyStopping callback\n",
    "history = model.fit([trainx, trainx_2, trainx_4, trainx_8], train_y, batch_size=10, epochs=500, verbose=0,\n",
    "                    validation_data=([validx, validx_2, validx_4, validx_8], valid_y), callbacks=[early_stopping])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 25)\n",
    "\n",
    "history = model.fit([trainx, trainx_2, trainx_4, trainx_8], train_y, batch_size=10, epochs=500, verbose=0,\n",
    "                validation_data=([validx, validx_2, validx_4, validx_8], valid_y), callbacks = [early_stopping])\n",
    "\n",
    "print('Training time: %s' % (datetime.now() - t))"
   ],
   "metadata": {
    "id": "Ux6TCyGX-5Ou"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the results\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'], 'b', label = 'Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.plot(history.history['val_loss'], 'r', label = 'Validation Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the minimum loss\n",
    "print(\"Training loss\", np.min(history.history['loss']))\n",
    "print(\"Validation loss\",np.min(history.history['val_loss']))"
   ],
   "metadata": {
    "id": "ylbCZ_oAMlg7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pydot\n",
    "\n",
    "# Install Graphviz library\n",
    "!pip install graphviz\n",
    "\n",
    "# Import the required modules\n",
    "import pydot\n",
    "import graphviz\n",
    "\n",
    "# Plot the prediction of the model for the training and validation sets\n",
    "pred_train = model.predict([trainx, trainx_2, trainx_4, trainx_8])\n",
    "\n",
    "pred_test = model.predict([validx, validx_2, validx_4, validx_8])\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(pred_train,'s', color='red', label='Prediction', linestyle='None', alpha = 0.5, markersize=6)\n",
    "plt.plot(train_y,'o', color='green',label='Quality Score', alpha = 0.4, markersize=6)\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.title('Training Set',fontsize=18)\n",
    "plt.xlabel('Sequence Number',fontsize=16)\n",
    "plt.ylabel('Quality Scale',fontsize=16)\n",
    "plt.legend(loc=3, prop={'size':14}) # loc:position\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(pred_test,'s', color='red', label='Prediction', linestyle='None', alpha = 0.5, markersize=6)\n",
    "plt.plot(valid_y,'o', color='green',label='Quality Score', alpha = 0.4, markersize=6)\n",
    "plt.title('Testing Set',fontsize=18)\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.xlabel('Sequence Number',fontsize=16)\n",
    "plt.ylabel('Quality Scale',fontsize=16)\n",
    "plt.legend(loc=3, prop={'size':14}) # loc:position\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/sample_data/Results/SpatioTemporalNN_Vicon_Scores.png', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "Z3e9oeN4Mvep"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate the cumulative deviation and rms deviation for the validation set\n",
    "test_dev = abs(np.squeeze(pred_test)-valid_y)\n",
    "# Cumulative deviation\n",
    "mean_abs_dev = np.mean(test_dev)\n",
    "# RMS deviation\n",
    "rms_dev = math.sqrt(mean_squared_error(pred_test, valid_y))\n",
    "print('Mean absolute deviation:', mean_abs_dev)\n",
    "print('RMS deviation:', rms_dev)"
   ],
   "metadata": {
    "id": "QsVv5g9mQ4At"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
