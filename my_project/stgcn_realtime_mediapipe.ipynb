{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 23:47:06.110773: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-01 23:47:06.250709: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-01 23:47:07.512052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# !pip install keras\n",
    "# !pip install tensorflow\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout, Dense, Input, LSTM, concatenate, ConvLSTM2D, Conv2D, Lambda, Reshape\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.nn import softmax, leaky_relu\n",
    "from tensorflow import expand_dims, einsum\n",
    "import tensorflow as tf\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "index_Spine_Base = 0\n",
    "index_Spine_Mid = 4\n",
    "index_Neck = 8\n",
    "index_Head = 12  # no orientation\n",
    "index_Shoulder_Left = 16\n",
    "index_Elbow_Left = 20\n",
    "index_Wrist_Left = 24\n",
    "index_Hand_Left = 28\n",
    "index_Shoulder_Right = 32\n",
    "index_Elbow_Right = 36\n",
    "index_Wrist_Right = 40\n",
    "index_Hand_Right = 44\n",
    "index_Hip_Left = 48\n",
    "index_Knee_Left = 52\n",
    "index_Ankle_Left = 56\n",
    "index_Foot_Left = 60  # no orientation\n",
    "index_Hip_Right = 64\n",
    "index_Knee_Right = 68\n",
    "index_Ankle_Right = 72\n",
    "index_Foot_Right = 76  # no orientation\n",
    "index_Spine_Shoulder = 80\n",
    "index_Tip_Left = 84  # no orientation\n",
    "index_Thumb_Left = 88  # no orientation\n",
    "index_Tip_Right = 92  # no orientation\n",
    "index_Thumb_Right = 96  # no orientation\n",
    "\n",
    "\n",
    "class Data_Loader:\n",
    "    def __init__(self, dir):\n",
    "        self.num_repitation = 5\n",
    "        self.num_channel = 3\n",
    "        self.dir = dir\n",
    "        self.body_part = self.body_parts()\n",
    "        self.dataset = []\n",
    "        self.sequence_length = []\n",
    "        self.num_timestep = 100\n",
    "        self.new_label = []\n",
    "        self.train_x, self.train_y = self.import_dataset()\n",
    "        self.batch_size = self.train_y.shape[0]\n",
    "        self.num_joints = len(self.body_part)\n",
    "        self.sc1 = StandardScaler()\n",
    "        self.sc2 = StandardScaler()\n",
    "        self.scaled_x, self.scaled_y = self.preprocessing()\n",
    "\n",
    "    def body_parts(self):\n",
    "        body_parts = [\n",
    "            index_Spine_Base,\n",
    "            index_Spine_Mid,\n",
    "            index_Neck,\n",
    "            index_Head,\n",
    "            index_Shoulder_Left,\n",
    "            index_Elbow_Left,\n",
    "            index_Wrist_Left,\n",
    "            index_Hand_Left,\n",
    "            index_Shoulder_Right,\n",
    "            index_Elbow_Right,\n",
    "            index_Wrist_Right,\n",
    "            index_Hand_Right,\n",
    "            index_Hip_Left,\n",
    "            index_Knee_Left,\n",
    "            index_Ankle_Left,\n",
    "            index_Foot_Left,\n",
    "            index_Hip_Right,\n",
    "            index_Knee_Right,\n",
    "            index_Ankle_Right,\n",
    "            index_Ankle_Right,\n",
    "            index_Spine_Shoulder,\n",
    "            index_Tip_Left,\n",
    "            index_Thumb_Left,\n",
    "            index_Tip_Right,\n",
    "            index_Thumb_Right,\n",
    "        ]\n",
    "        return body_parts\n",
    "\n",
    "    def import_dataset(self):\n",
    "        train_x = (\n",
    "            pd.read_csv(\"./\" + self.dir + \"/Train_X.csv\", header=None).iloc[:, :].values\n",
    "        )\n",
    "        train_y = (\n",
    "            pd.read_csv(\"./\" + self.dir + \"/Train_Y.csv\", header=None).iloc[:, :].values\n",
    "        )\n",
    "        return train_x, train_y\n",
    "\n",
    "    def preprocessing(self):\n",
    "        X_train = np.zeros(\n",
    "            (self.train_x.shape[0], self.num_joints * self.num_channel)\n",
    "        ).astype(\"float32\")\n",
    "        for row in range(self.train_x.shape[0]):\n",
    "            counter = 0\n",
    "            for parts in self.body_part:\n",
    "                for i in range(self.num_channel):\n",
    "                    X_train[row, counter + i] = self.train_x[row, parts + i]\n",
    "                counter += self.num_channel\n",
    "\n",
    "        y_train = np.reshape(self.train_y, (-1, 1))\n",
    "        X_train = self.sc1.fit_transform(X_train)\n",
    "        y_train = self.sc2.fit_transform(y_train)\n",
    "\n",
    "        X_train_ = np.zeros(\n",
    "            (self.batch_size, self.num_timestep, self.num_joints, self.num_channel)\n",
    "        )\n",
    "\n",
    "        for batch in range(X_train_.shape[0]):\n",
    "            for timestep in range(X_train_.shape[1]):\n",
    "                for node in range(X_train_.shape[2]):\n",
    "                    for channel in range(X_train_.shape[3]):\n",
    "                        X_train_[batch, timestep, node, channel] = X_train[\n",
    "                            timestep + (batch * self.num_timestep),\n",
    "                            channel + (node * self.num_channel),\n",
    "                        ]\n",
    "\n",
    "        X_train = X_train_\n",
    "        return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Graph():\n",
    "    def __init__(self, num_node):\n",
    "        self.num_node = num_node\n",
    "        self.AD, self.AD2, self.bias_mat_1, self.bias_mat_2 = self.normalize_adjacency()\n",
    "        \n",
    "    def normalize_adjacency(self):\n",
    "        self_link = [(i, i) for i in range(self.num_node)]\n",
    "        neighbor_1base = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n",
    "                                      (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n",
    "                                      (11, 10), (12, 11), (13, 1), (14, 13), (15, 14),\n",
    "                                      (16, 15), (17, 1), (18, 17), (19, 18), (20, 19),\n",
    "                                      (22, 23), (23, 8), (24, 25), (25, 12)]\n",
    "        neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n",
    "        edge = self_link + neighbor_link    \n",
    "        A = np.zeros((self.num_node, self.num_node)) # adjacency matrix\n",
    "        for i, j in edge:\n",
    "            A[j, i] = 1\n",
    "            A[i, j] = 1\n",
    "        \n",
    "        A2 = np.zeros((self.num_node, self.num_node)) # second order adjacency matrix\n",
    "        for root in range(A.shape[1]):\n",
    "            for neighbour in range(A.shape[0]):\n",
    "                if A[root, neighbour] == 1:\n",
    "                    for neighbour_of_neigbour in range(A.shape[0]):\n",
    "                        if A[neighbour, neighbour_of_neigbour] == 1:\n",
    "                            A2[root,neighbour_of_neigbour] = 1                 \n",
    "        #AD = self.normalize(A)\n",
    "        #AD2 = self.normalize(A2)\n",
    "        bias_mat_1 = np.zeros(A.shape)\n",
    "        bias_mat_2 = np.zeros(A2.shape)\n",
    "        bias_mat_1 = np.where(A!=0, bias_mat_1, -1e9)\n",
    "        bias_mat_2 = np.where(A2!=0, A2, -1e9)\n",
    "        AD = A.astype('float32')\n",
    "        AD2 = A2.astype('float32')\n",
    "        bias_mat_1 = bias_mat_1.astype('float32')\n",
    "        bias_mat_2 = bias_mat_2.astype('float32')\n",
    "        AD = tf.convert_to_tensor(AD)\n",
    "        AD2= tf.convert_to_tensor(AD2)\n",
    "        bias_mat_1 = tf.convert_to_tensor(bias_mat_1)\n",
    "        bias_mat_2 = tf.convert_to_tensor(bias_mat_2)\n",
    "        return AD, AD2, bias_mat_1, bias_mat_2\n",
    "        \n",
    "    def normalize(self, adjacency):\n",
    "        rowsum = np.array(adjacency.sum(1))\n",
    "        r_inv = np.power(rowsum, -1).flatten()\n",
    "        r_inv[np.isinf(r_inv)] = 0\n",
    "        r_mat_inv = np.diag(r_inv)\n",
    "        normalize_adj = r_mat_inv.dot(adjacency)\n",
    "        normalize_adj = normalize_adj.astype('float32')\n",
    "        normalize_adj = tf.convert_to_tensor(normalize_adj)   \n",
    "        return normalize_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GCNLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        AD, x = inputs\n",
    "        return tf.einsum('vw,ntwc->ntvc', AD, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sgcn_Lstm():\n",
    "    def __init__(self, train_x, train_y, valid_x, valid_y, AD, AD2, lr=0.0001, epoach=200, batch_size=10):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.valid_x = valid_x\n",
    "        self.valid_y = valid_y\n",
    "        self.AD = AD\n",
    "        self.AD2 = AD2\n",
    "        self.lr = lr\n",
    "        self.epoach =epoach\n",
    "        self.batch_size = batch_size\n",
    "        self.num_joints = 25\n",
    "\n",
    "\n",
    "    def _conv_layer(self, Input, filters):\n",
    "        x = Conv2D(filters=filters, kernel_size=(1,1), strides=1, activation='relu')(Input)\n",
    "        x = Dropout(0.25)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _gcn_layer(self, AD, x):\n",
    "        # gcn = tf.keras.layers.Lambda(lambda x: tf.einsum('vw,ntwc->ntvc', x[0], x[1]))([AD, x])\n",
    "        gcn = GCNLayer()([AD, x])\n",
    "        return gcn\n",
    "\n",
    "    def sgcn(self, Input):\n",
    "        x = self._conv_layer(Input, 64)\n",
    "        gcn_1 = self._gcn_layer(self.AD, x)\n",
    "        y = self._conv_layer(Input, 64)\n",
    "        gcn_2 = self._gcn_layer(self.AD2, y)\n",
    "        concatenated_gcn_1_2 = concatenate([gcn_1, gcn_2], axis=-1)\n",
    "\n",
    "        x = self._conv_layer(concatenated_gcn_1_2, 128)\n",
    "        gcn_3 = self._gcn_layer(self.AD, x)\n",
    "        y = self._conv_layer(concatenated_gcn_1_2, 128)\n",
    "        gcn_4 = self._gcn_layer(self.AD2, y)\n",
    "        concatenated_gcn_3_4 = concatenate([gcn_3, gcn_4], axis=-1)\n",
    "\n",
    "        gcn = tf.keras.layers.Reshape(target_shape=(-1,concatenated_gcn_3_4.shape[2]*concatenated_gcn_3_4.shape[3]))(concatenated_gcn_3_4)\n",
    "\n",
    "        return gcn\n",
    "\n",
    "    def Lstm(self,x):\n",
    "        rec = LSTM(80, return_sequences=True)(x)\n",
    "        rec = Dropout(0.25)(rec)\n",
    "        rec1 = LSTM(40, return_sequences=True)(rec)\n",
    "        rec1 = Dropout(0.25)(rec1)\n",
    "        rec2 = LSTM(40, return_sequences=True)(rec1)\n",
    "        rec2 = Dropout(0.25)(rec2)\n",
    "        rec3 = LSTM(80)(rec2)\n",
    "        rec3 = Dropout(0.25)(rec3)\n",
    "        return Dense(1, activation = 'linear')(rec3)\n",
    "\n",
    "    def build(self):\n",
    "        seq_input = Input(shape=(None, self.train_x.shape[2], self.train_x.shape[3]), batch_size=None)\n",
    "        sgcn_layer = self.sgcn(seq_input)\n",
    "        lstm_sgcn_layer = self.Lstm(sgcn_layer)\n",
    "        self.model = Model(seq_input, lstm_sgcn_layer)\n",
    "        return self.model\n",
    "\n",
    "    def train(self):\n",
    "        t = dt.now()\n",
    "\n",
    "        model = self.build()\n",
    "        model.compile(loss=tf.keras.losses.Huber(delta=0.1), optimizer= Adam(learning_rate=self.lr))\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience = 25)\n",
    "        checkpoint = ModelCheckpoint(\"models/model_ex5.keras\", monitor='val_loss', save_best_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "        history = self.model.fit(\n",
    "            self.train_x,\n",
    "            self.train_y,\n",
    "            validation_data = (self.valid_x,self.valid_y),\n",
    "            epochs=self.epoach,\n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[checkpoint, early_stopping]\n",
    "            )\n",
    "\n",
    "        print('Training time: %s' % (dt.now() - t))\n",
    "\n",
    "        self.model.save(\"models/my_model_trained_exercise.keras\")\n",
    "\n",
    "        return history\n",
    "\n",
    "    def load_wights(self, file_path):\n",
    "      self.model.load_wights(file_path)\n",
    "\n",
    "    def save(self, file_path=\"models/my_model_trained_exercise.keras\"):\n",
    "        self.model.save(file_path)\n",
    "\n",
    "    def prediction(self, data):\n",
    "        return self.model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# from data_processing import Data_Loader\n",
    "# from graph import Graph\n",
    "# from GCN.sgcn_lstm import Sgcn_Lstm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "random_seed = 42  # for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training instances:  298\n",
      "Validation instances:  75\n"
     ]
    }
   ],
   "source": [
    "exercise = 'data/KIMORE/Kimore_ex5'\n",
    "learning_rate = 0.0001\n",
    "# epoch = 500\n",
    "epoch = 5\n",
    "batch_size = 10\n",
    "\n",
    "\"\"\"import the whole dataset\"\"\"\n",
    "data_loader = Data_Loader(exercise)  # folder name -> Train.csv, Test.csv\n",
    "\n",
    "\"\"\"import the graph data structure\"\"\"\n",
    "graph = Graph(len(data_loader.body_part))\n",
    "\n",
    "\"\"\"Split the data into training and validation sets while preserving the distribution\"\"\"\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(data_loader.scaled_x, data_loader.scaled_y, test_size=0.2, random_state = random_seed)\n",
    "\n",
    "print(\"Training instances: \", len(train_x))\n",
    "print(\"Validation instances: \", len(valid_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the algorithm\"\"\"\n",
    "# algorithm = Sgcn_Lstm(train_x, train_y, graph.AD, graph.AD2, graph.bias_mat_1, graph.bias_mat_2, lr = args.lr, epoach=args.epoch, batch_size=args.batch_size)\n",
    "algorithm = Sgcn_Lstm(train_x, train_y, valid_x, valid_y, graph.AD, graph.AD2, lr = learning_rate, epoach=epoch, batch_size=batch_size)\n",
    "model = algorithm.build()\n",
    "history = algorithm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# algorithm_loaded = Sgcn_Lstm(train_x, train_y, valid_x, valid_y, graph.AD, graph.AD2, lr = learning_rate, epoach=epoch, batch_size=batch_size)\n",
    "# model_loaded = algorithm_loaded.build()\n",
    "# model_loaded.load_weights(\"models/my_model_trained_exercise.weights.h5\")\n",
    "# model_loaded.load(\"models/model_ex5.keras\")\n",
    "\n",
    "custom_objects = {'GCNLayer': GCNLayer}\n",
    "full_model = tf.keras.models.load_model('models/my_model_trained_exercise.keras', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451ms/step\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test the model\"\"\"\n",
    "# y_pred = algorithm.prediction(valid_x)\n",
    "y_pred = full_model.predict(valid_x)\n",
    "y_pred = data_loader.sc2.inverse_transform(y_pred)\n",
    "valid_y = data_loader.sc2.inverse_transform(valid_y) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'algorithm_loaded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm_loaded\u001b[49m\u001b[38;5;241m.\u001b[39mprediction(valid_x)\n\u001b[1;32m      2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39msc2\u001b[38;5;241m.\u001b[39minverse_transform(y_pred)\n\u001b[1;32m      3\u001b[0m valid_y \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39msc2\u001b[38;5;241m.\u001b[39minverse_transform(valid_y) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'algorithm_loaded' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = algorithm_loaded.prediction(valid_x)\n",
    "y_pred = data_loader.sc2.inverse_transform(y_pred)\n",
    "valid_y = data_loader.sc2.inverse_transform(valid_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Performance matric\"\"\"\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute deviation: 5.138071108324562\n",
      "RMS deviation: 7.124730403959575\n",
      "MSE: 50.76178332910597\n",
      "MAPE:  18.597278047983977\n"
     ]
    }
   ],
   "source": [
    "test_dev = abs(valid_y-y_pred)\n",
    "mean_abs_dev = np.mean(test_dev)\n",
    "mae = mean_absolute_error(valid_y, y_pred)\n",
    "rms_dev = sqrt(mean_squared_error(y_pred, valid_y))\n",
    "mse = mean_squared_error(valid_y,y_pred) \n",
    "mape = mean_absolute_percentage_error(valid_y, y_pred)\n",
    "\n",
    "print('Mean absolute deviation:', mae)\n",
    "print('RMS deviation:', rms_dev)\n",
    "print('MSE:', mse)\n",
    "print('MAPE: ', mape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
