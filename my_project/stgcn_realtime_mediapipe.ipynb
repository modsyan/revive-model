{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Dense, Input, LSTM, concatenate, ConvLSTM2D, Conv2D, Lambda, Reshape\n",
    "from keras.losses import Huber\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.nn import softmax, leaky_relu\n",
    "from tensorflow import expand_dims, einsum\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sgcn_Lstm():\n",
    "    def __init__(self, train_x, train_y, valid_x, valid_y, AD, AD2, lr=0.0001, epoach=200, batch_size=10):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.valid_x = valid_x\n",
    "        self.valid_y = valid_y\n",
    "        self.AD = AD\n",
    "        self.AD2 = AD2\n",
    "        self.lr = lr\n",
    "        self.epoach =epoach\n",
    "        self.batch_size = batch_size\n",
    "        self.num_joints = 25\n",
    "\n",
    "    def sgcn(self, Input):\n",
    "        x = tf.keras.layers.Conv2D(filters=64, kernel_size=(1,1), strides=1, activation='relu')(Input)\n",
    "        x = Dropout(0.25)(x)\n",
    "        gcn_1 = tf.keras.layers.Lambda(lambda x: tf.einsum('vw,ntwc->ntvc', x[0], x[1]))([self.AD, x])\n",
    "        y = tf.keras.layers.Conv2D(filters=64, kernel_size=(1,1), strides=1, activation='relu')(Input)\n",
    "        y = Dropout(0.25)(y)\n",
    "        gcn_2 = tf.keras.layers.Lambda(lambda x: tf.einsum('vw,ntwc->ntvc', x[0], x[1]))([self.AD2, y])\n",
    "        gcn = concatenate([gcn_1, gcn_2], axis=-1)                                                                                                                                   \n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters=128, kernel_size=(1,1), strides=1, activation='relu')(gcn)\n",
    "        x = Dropout(0.25)(x)\n",
    "        gcn_1 = tf.keras.layers.Lambda(lambda x: tf.einsum('vw,ntwc->ntvc', x[0], x[1]))([self.AD, x])\n",
    "        y = tf.keras.layers.Conv2D(filters=128, kernel_size=(1,1), strides=1, activation='relu')(gcn)\n",
    "        y = Dropout(0.25)(y)\n",
    "        gcn_2 = tf.keras.layers.Lambda(lambda x: tf.einsum('vw,ntwc->ntvc', x[0], x[1]))([self.AD2, y])\n",
    "        gcn = concatenate([gcn_1, gcn_2], axis=-1)\n",
    "        \n",
    "        gcn = tf.keras.layers.Reshape(target_shape=(-1,gcn.shape[2]*gcn.shape[3]))(gcn)\n",
    "        return gcn\n",
    "\n",
    "    def Lstm(self,x):\n",
    "        rec = LSTM(80, return_sequences=True)(x)\n",
    "        rec = Dropout(0.25)(rec)\n",
    "        rec1 = LSTM(40, return_sequences=True)(rec)\n",
    "        rec1 = Dropout(0.25)(rec1)\n",
    "        rec2 = LSTM(40, return_sequences=True)(rec1)\n",
    "        rec2 = Dropout(0.25)(rec2)\n",
    "        rec3 = LSTM(80)(rec2)\n",
    "        rec3 = Dropout(0.25)(rec3)\n",
    "        output = Dense(1, activation = 'linear')(rec3)\n",
    "        return output\n",
    "\n",
    "    def build(self):\n",
    "        seq_input = Input(shape=(None, self.train_x.shape[2], self.train_x.shape[3]), batch_size=None)\n",
    "        sgcn_layer = self.sgcn(seq_input)\n",
    "        lstm_sgcn_layer = self.Lstm(sgcn_layer)\n",
    "        self.model = Model(seq_input, lstm_sgcn_layer)\n",
    "        return self.model     \n",
    "\n",
    "    def train(self):\n",
    "        self.model.compile(loss=tf.keras.losses.Huber(delta=0.1), optimizer= Adam(learning_rate=self.lr))\n",
    "        checkpoint = ModelCheckpoint(\"models/model_ex5.keras\", monitor='val_loss', save_best_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "        history = self.model.fit(self.train_x, self.train_y, validation_data = (self.valid_x,self.valid_y), epochs=self.epoach, batch_size=self.batch_size, callbacks=[checkpoint])\n",
    "        return history\n",
    "    \n",
    "    def prediction(self, data):\n",
    "        y_pred = self.model.predict(data)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from data_processing import Data_Loader\n",
    "from graph import Graph\n",
    "# from GCN.sgcn_lstm import Sgcn_Lstm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_seed = 42  # for reproducibility\n",
    "\n",
    "# # Create the parser\n",
    "# my_parser = argparse.ArgumentParser(description='List of argument')\n",
    "\n",
    "# # Add the arguments\n",
    "# my_parser.add_argument('--ex', type=str, default='Kimore_ex5', help='the name of exercise.', required=True)\n",
    "# my_parser.add_argument('--lr', type=int, default= 0.0001, help='initial learning rate for optimizer.')\n",
    "# my_parser.add_argument('--epoch', type=int, default= 1000, help='number of epochs to train.')\n",
    "# my_parser.add_argument('--batch-size', type=int, default= 10, help='training batch size.')\n",
    "\n",
    "# # Execute the parse_args() method\n",
    "# args = my_parser.parse_args()\n",
    "\n",
    "exercise = 'data/KIMORE/Kimore_ex5'\n",
    "learning_rate = 0.0001\n",
    "epoch = 1000\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training instances:  298\n",
      "Validation instances:  75\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The filepath provided must end in `.keras` (Keras model format). Received: filepath=models/best_model_ex1.hdf5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m algorithm \u001b[38;5;241m=\u001b[39m Sgcn_Lstm(train_x, train_y, valid_x, valid_y, graph\u001b[38;5;241m.\u001b[39mAD, graph\u001b[38;5;241m.\u001b[39mAD2, lr \u001b[38;5;241m=\u001b[39m learning_rate, epoach\u001b[38;5;241m=\u001b[39mepoch, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m algorithm\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[0;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 55\u001b[0m, in \u001b[0;36mSgcn_Lstm.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mHuber(delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m), optimizer\u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr))\n\u001b[0;32m---> 55\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/best_model_ex1.hdf5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_y, validation_data \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_x,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_y), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoach, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, callbacks\u001b[38;5;241m=\u001b[39m[checkpoint])\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/work/graduation/automatic_evaluation_of_physical_therapy_exercises_based_on_deep_learning/.venv/lib/python3.11/site-packages/keras/src/callbacks/model_checkpoint.py:191\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[0;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filepath provided must end in `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras model format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=models/best_model_ex1.hdf5"
     ]
    }
   ],
   "source": [
    "\"\"\"import the whole dataset\"\"\"\n",
    "data_loader = Data_Loader(exercise)  # folder name -> Train.csv, Test.csv\n",
    "\n",
    "\"\"\"import the graph data structure\"\"\"\n",
    "graph = Graph(len(data_loader.body_part))\n",
    "\n",
    "\"\"\"Split the data into training and validation sets while preserving the distribution\"\"\"\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(data_loader.scaled_x, data_loader.scaled_y, test_size=0.2, random_state = random_seed)\n",
    "\n",
    "print(\"Training instances: \", len(train_x))\n",
    "print(\"Validation instances: \", len(valid_x))\n",
    "\n",
    "\"\"\"Train the algorithm\"\"\"\n",
    "# algorithm = Sgcn_Lstm(train_x, train_y, graph.AD, graph.AD2, graph.bias_mat_1, graph.bias_mat_2, lr = args.lr, epoach=args.epoch, batch_size=args.batch_size)\n",
    "algorithm = Sgcn_Lstm(train_x, train_y, valid_x, valid_y, graph.AD, graph.AD2, lr = learning_rate, epoach=epoch, batch_size=batch_size)\n",
    "model = algorithm.build()\n",
    "history = algorithm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test the model\"\"\"\n",
    "y_pred = algorithm.prediction(valid_x)\n",
    "y_pred = data_loader.sc2.inverse_transform(y_pred)\n",
    "valid_y = data_loader.sc2.inverse_transform(valid_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Performance matric\"\"\"\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dev = abs(valid_y-y_pred)\n",
    "mean_abs_dev = np.mean(test_dev)\n",
    "mae = mean_absolute_error(valid_y, y_pred)\n",
    "rms_dev = sqrt(mean_squared_error(y_pred, valid_y))\n",
    "mse = mean_squared_error(valid_y,y_pred) \n",
    "mape = mean_absolute_percentage_error(valid_y, y_pred)\n",
    "\n",
    "print('Mean absolute deviation:', mae)\n",
    "print('RMS deviation:', rms_dev)\n",
    "print('MSE:', mse)\n",
    "print('MAPE: ', mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
